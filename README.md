# ğŸ” FastAPI Semantic Search & RAG Backend

This project provides a semantic search and Retrieval-Augmented Generation (RAG) backend using:

- **FastAPI** for APIs  
- **ChromaDB** as the vector store  
- **SentenceTransformers (all-mpnet-base-v2)** for embeddings  
- **Ollama** as the local LLM runtime  
- **Punkt sentence chunking** with token overlap  
- **Deduplication logic** to prevent redundant chunk storage

---

## ğŸ“¦ Features

- âœ… Upload `.txt` files for semantic indexing
- âœ… Automatic sentence-aware chunking
- âœ… Duplicate chunk prevention
- âœ… Semantic vector search using sentence embeddings
- âœ… RAG-style response generation using local LLM (Ollama)
- âœ… FastAPI-based REST endpoints

---

## âš™ï¸ Setup Instructions

### 1. Clone the repo

```bash
git clone https://github.com/Ajinkya-A3/RAG-Model.git
cd RAG-Model
```

### 2. Create and activate virtual environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

> Make sure you have `chromadb`, `fastapi`, `uvicorn`, `nltk`, and `sentence-transformers` installed.

### 4. Download Punkt tokenizer for NLTK

This is handled automatically by the app. You can also manually run:

```bash
python -m nltk.downloader punkt
```

---

## ğŸš€ Start the Backend

```bash
uvicorn main:app --reload
```

The server will start at `http://127.0.0.1:8000`

---

## ğŸ§ª API Endpoints

### ğŸŸ¢ `GET /`
Liveness probe.  
**Use:** Check if the app is running.  
**Response:** `{ "status": "alive" }`

---

### ğŸŸ¢ `GET /ready`
Readiness probe.  
**Use:** Confirms ChromaDB is reachable and initialized.  
**Response:** `{ "status": "ready" }`

---

### ğŸ“„ `POST /upload`
Upload a `.txt` file. Automatically chunks the file by sentences and stores the vectors in ChromaDB, avoiding duplicates.

**Request:**  
Content-Type: `multipart/form-data`  
Field: `file` = `.txt` file

**Response (Example):**
```json
{
  "status": "uploaded",
  "filename": "devops.txt",
  "added_chunks": 17,
  "skipped_duplicates": 3
}
```

---

### ğŸ” `POST /search`
Semantic search on uploaded data.

**Request Body:**
```json
{
  "prompt": "How do I set environment variables in Docker?"
}
```

**Response:**
```json
{
  "query": "How do I set environment variables in Docker?",
  "matches": [
    {
      "document": "...matched content...",
      "id": "docker_guide_chunk_2",
      "distance": 0.13
    }
  ]
}
```

---

### ğŸ’¬ `POST /rag`
Retrieves relevant chunks and passes them to a local LLM (Ollama) for a natural language response.

**Request Body:**
```json
{
  "prompt": "Explain how Kubernetes probes work"
}
```

**Response:**
```json
{
  "context": "...top matching chunks...",
  "answer": "Kubernetes uses liveness and readiness probes to determine..."
}
```

---

### ğŸ“š `GET /records`
Returns all stored ChromaDB records.

**Response:**
```json
{
  "count": 42,
  "ids": ["devops_chunk_0", "devops_chunk_1", "..."],
  "documents": ["First chunk", "Second chunk", "..."]
}
```

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ main.py               # FastAPI app with all endpoints
â”œâ”€â”€ chroma_setup.py       # Chunking, embedding, and ChromaDB logic
â”œâ”€â”€ rag.py                # RAG + Ollama interaction
â”œâ”€â”€ data/                 # Uploaded text files
â”œâ”€â”€ chroma_db/            # Persistent ChromaDB storage
â”œâ”€â”€ nltk_data/            # NLTK punkt tokenizer
â”œâ”€â”€ transformer_model/    # HuggingFace model cache
```

---

## ğŸ§ª Test with cURL

**Upload a file:**
```bash
curl -X POST "http://127.0.0.1:8000/upload" -F "file=@yourfile.txt"
```

**Query with search:**
```bash
curl -X POST "http://127.0.0.1:8000/search" -H "Content-Type: application/json" -d '{"prompt": "What is a Kubernetes probe?"}'
```

---

## ğŸ‘¤ Author

[Ajinkya A3](https://github.com/Ajinkya-A3)

GitHub Repo: [RAG-Model](https://github.com/Ajinkya-A3/RAG-Model/blob/main/app/main.py)

---

## âš–ï¸ License

```
Apache License 2.0

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0
```